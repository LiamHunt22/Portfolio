import scapy.all as sc
import numpy as np
import os
import sys
import itertools
import pandas as pd
from keras.utils.np_utils import to_categorical
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from datetime import datetime

###################################################################################
# run as 'python3 read_pcap.py ../data/PCAP/ n'
# replace n with number of values to pull (20, 500, 900, ...)
# does not create y_train_raw -- I would imagine this would be the same as y_train?
###################################################################################

LABEL2DIG = {'chat': 0, 'voip': 1, 'trap2p': 2, 'stream': 3, 'file_trans': 4, 'email': 5}
DIG2LABEL = {v: k for k, v in LABEL2DIG.items()}
num_classes = 12
ip_header_size = 60

# n is the size of a sample in bytes
def create_row(data, n):
    raw_values_t = []
    for packet in data:
        if packet.haslayer(sc.Raw):
            #each list in raw_values_t is a list of the bytes in that packet
            raw_values_t.append(np.frombuffer(packet.load, dtype= np.uint8))
    #convert from list of lists to list of bytes
    raw_values = list(itertools.chain(*raw_values_t))
    print("The size of the raw bytes of the flow is: ", len(raw_values))

    #partition raw_values into multiple samples
    flow_header = np.array(raw_values[0:ip_header_size])
    payload_size = n - ip_header_size
    num_samples = (len(raw_values) - ip_header_size) // payload_size
    padding_len = payload_size - (len(raw_values) - ip_header_size) % payload_size
    #print("The padding length is: ", padding_len)
    if padding_len != payload_size:
        num_samples += 1
    print("The number of samples generated by the flow is: ", num_samples)
    raw_paddedPayload_1D = np.pad(raw_values, (0, padding_len), 'constant')
    raw_paddedPayload_1D = np.delete(raw_paddedPayload_1D, np.s_[:ip_header_size])
    raw_paddedPayload = raw_paddedPayload_1D.reshape(num_samples, payload_size)
    #print("The size of the 2D raw payload is", len(raw_paddedPayload), " by ", len(raw_paddedPayload[0]))
    num_samples = min(280, num_samples)
    samples = np.empty(shape=(num_samples,n))
    for i in range(num_samples):    # only take the first 160 samples for each flow
        if n <= ip_header_size:
            samples[i, :n] = flow_header[0:n]
        else:
            samples[i, :ip_header_size] = flow_header
            samples[i, ip_header_size : n] = raw_paddedPayload[i,:]
    samples = list(samples)

    # payload_start_loc = ip_header_size
    # num_samples = 0
    # for raw_values[payload_start_loc : (payload_start_loc + payload_size)] in raw_values:
    #     if n <= ip_header_size:
    #         samples[num_samples,:ip_header_size] = flow_header[0:ip_header_size]
    #     else:
    #         samples[num_samples,:ip_header_size] = flow_header
    #         samples[num_samples,ip_header_size : n] = raw_values[payload_start_loc : (payload_start_loc + payload_size)]
    #         payload_start_loc = payload_size + payload_size
    #     num_samples += 1
    # # for the remaining bytes at the end of the flow
    # if payload_start_loc < len(raw_values):
    #     remaining_len = len(raw_values) - payload_start_loc
    #     samples[num_samples,:remaining_len] = raw_values[payload_start_loc : len(raw_values)]
    #     samples[num_samples,remaining_len :] = 0

    return samples

if __name__ == "__main__":
    #source = sys.argv[1]
    #source = '../data/PCAP/'
    source = '/media/qian/DATA/my_research/Network_Traffic_Classification/network_flow_data/ISCXVPN2016-UNB/VPN-PCAPs-02/'
    n = int(sys.argv[2])    # n = 11, 599, 937, ...

    X_table = []
    y_table_word = []
    y_table_num = []

    # y_train = np.load('../data/y_train.npy', allow_pickle=True) #0,1,2,...,11
    # print(y_train[:100])

    for (root, dirs, file) in os.walk(source):
        for f in file:
            print(f)
            # X_train.npy
            if '.pcap' or '.pcapng' in f:
                flow_samples = create_row(sc.rdpcap(source + f), n)
                X_table.extend(flow_samples)
                num_samples = len(flow_samples)
                print("The number of samples taken from the flow is: ", num_samples)
            # y_train.npy in words
            if 'chat' in f and 'vpn' in f:         # type 6
                append_word = np.array([['vpn_chat'] * num_samples]).T.tolist()
                append_num = np.array([[6] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'audio' in f and 'vpn' in f:        # type 7
                append_word = np.array([['vpn_voip'] * num_samples]).T.tolist()
                append_num = np.array([[7] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'p2p' in f and 'vpn' in f:          # type 8
                append_word = np.array([['vpn_trap2p'] * num_samples]).T.tolist()
                append_num = np.array([[8] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif ('video' in f or 'stream' in f) and ('vpn' in f):      # type 9
                append_word = np.array([['vpn_stream'] * num_samples]).T.tolist()
                append_num = np.array([[9] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif ('ftp' in f or 'scp' in f or 'file' in f) and ('vpn' in f): # type 10
                append_word = np.array([['vpn_file_trans'] * num_samples]).T.tolist()
                append_num = np.array([[10] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'email' in f and 'vpn' in f:                  # type 11
                append_word = np.array([['vpn_email'] * num_samples]).T.tolist()
                append_num = np.array([[11] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'chat' in f:                                   # type 0
                append_word = np.array([['chat'] * num_samples]).T.tolist()
                append_num = np.array([[0] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'audio' in f:                                  # type 1
                append_word = np.array([['voip'] * num_samples]).T.tolist()
                append_num = np.array([[1] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'p2p' in f:                                    # type 2
                append_word = np.array([['trap2p'] * num_samples]).T.tolist()
                append_num = np.array([[2] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'video' in f or 'stream' in f:      # type 3
                append_word = np.array([['stream'] * num_samples]).T.tolist()
                append_num = np.array([[3] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'ftp' in f or 'scp' in f or 'file' in f: # type 4
                append_word = np.array([['file_trans'] * num_samples]).T.tolist()
                append_num = np.array([[4] * num_samples]).T.tolist()         
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            elif 'email' in f:                  # type 5
                append_word = np.array([['email'] * num_samples]).T.tolist()
                append_num = np.array([[5] * num_samples]).T.tolist()          
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)
            else:
                append_word = np.array([['unrecognized'] * num_samples]).T.tolist()
                append_num = np.array([[999] * num_samples]).T.tolist()         
                y_table_word.extend(append_word)
                y_table_num.extend(append_num)


    # look at the data
    print('X_table dimension:', np.shape(X_table))
    print('y_table dimension:', np.shape(y_table_num))
    #print("X_table is: ", X_table)
    #print("y_table is (words): ", y_table_word)
    #print("y_table is (numbers): ", y_table_num)

    # write into csv files
    df = pd.DataFrame(X_table)
    X_CSVName = 'X_raw_' + str(n) + '.csv'
    df.to_csv(X_CSVName, mode='a', index=False, header=None)
    df = pd.DataFrame(y_table_num)
    y_CSVName = 'y_raw_' + str(n) + '.csv'
    df.to_csv(y_CSVName , mode='a', index=False, header=None)

    # read all data from csv files and save it as npy files
    X_table = pd.read_csv(X_CSVName)
    y_table_num = pd.read_csv(y_CSVName)

    X_npyName = 'X_train_raw_' + str(n)
    y_npyName = 'y_train_raw_' + str(n)
    np.save(X_npyName, X_table)
    np.save(y_npyName, y_table_num)
    print("The size of ", X_npyName,".npy file is: ", np.shape(X_table))
    print("The size of ", y_npyName,".npy file is: ", np.shape(y_table_num))